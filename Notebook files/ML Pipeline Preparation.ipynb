{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import library\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import ne_chunk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import tree2conlltags\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "      <th>not_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>Storm at sacred heart of jesus</td>\n",
       "      <td>Cyclone Coeur sacr de jesus</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>Please, we need tents and water. We are in Sil...</td>\n",
       "      <td>Tanpri nou bezwen tant avek dlo nou zon silo m...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>I am in Croix-des-Bouquets. We have health iss...</td>\n",
       "      <td>Nou kwadebouke, nou gen pwoblem sant m yo nan ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>There's nothing to eat and water, we starving ...</td>\n",
       "      <td>Bon repo pa gen anyen menm grangou swaf</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22</td>\n",
       "      <td>I am in Thomassin number 32, in the area named...</td>\n",
       "      <td>Mwen thomassin 32 nan pyron mwen ta renmen jwe...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24</td>\n",
       "      <td>Let's do it together, need food in Delma 75, i...</td>\n",
       "      <td>Ann fel ansanm bezwen manje nan delma 75 nan r...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26</td>\n",
       "      <td>A Comitee in Delmas 19, Rue ( street ) Janvier...</td>\n",
       "      <td>Komite katye delma 19 rue janvier imp charite ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27</td>\n",
       "      <td>We need food and water in Klecin 12. We are dy...</td>\n",
       "      <td>Nou bezwen mange avek dlo nan klcin 12 LA LAFI...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   7            Is the Hurricane over or is it not over   \n",
       "1   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "2  15                     Storm at sacred heart of jesus   \n",
       "3  16  Please, we need tents and water. We are in Sil...   \n",
       "4  18  I am in Croix-des-Bouquets. We have health iss...   \n",
       "5  20  There's nothing to eat and water, we starving ...   \n",
       "6  22  I am in Thomassin number 32, in the area named...   \n",
       "7  24  Let's do it together, need food in Delma 75, i...   \n",
       "8  26  A Comitee in Delmas 19, Rue ( street ) Janvier...   \n",
       "9  27  We need food and water in Klecin 12. We are dy...   \n",
       "\n",
       "                                            original   genre  related  \\\n",
       "0                 Cyclone nan fini osinon li pa fini  direct        1   \n",
       "1  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "2                        Cyclone Coeur sacr de jesus  direct        1   \n",
       "3  Tanpri nou bezwen tant avek dlo nou zon silo m...  direct        1   \n",
       "4  Nou kwadebouke, nou gen pwoblem sant m yo nan ...  direct        1   \n",
       "5            Bon repo pa gen anyen menm grangou swaf  direct        1   \n",
       "6  Mwen thomassin 32 nan pyron mwen ta renmen jwe...  direct        1   \n",
       "7  Ann fel ansanm bezwen manje nan delma 75 nan r...  direct        1   \n",
       "8  Komite katye delma 19 rue janvier imp charite ...  direct        1   \n",
       "9  Nou bezwen mange avek dlo nan klcin 12 LA LAFI...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products     ...       \\\n",
       "0        0      0            1             0                 0     ...        \n",
       "1        1      0            1             0                 1     ...        \n",
       "2        0      0            0             0                 0     ...        \n",
       "3        1      0            1             0                 0     ...        \n",
       "4        1      0            1             1                 1     ...        \n",
       "5        1      0            1             1                 1     ...        \n",
       "6        1      0            1             0                 0     ...        \n",
       "7        1      0            1             0                 0     ...        \n",
       "8        1      0            1             0                 1     ...        \n",
       "9        1      0            1             1                 0     ...        \n",
       "\n",
       "   other_infrastructure  weather_related  floods  storm  fire  earthquake  \\\n",
       "0                     0                1       0      1     0           0   \n",
       "1                     0                0       0      0     0           0   \n",
       "2                     0                1       0      1     0           0   \n",
       "3                     0                0       0      0     0           0   \n",
       "4                     0                0       0      0     0           0   \n",
       "5                     1                1       1      0     0           0   \n",
       "6                     0                0       0      0     0           0   \n",
       "7                     0                0       0      0     0           0   \n",
       "8                     0                0       0      0     0           0   \n",
       "9                     0                0       0      0     0           0   \n",
       "\n",
       "   cold  other_weather  direct_report  not_related  \n",
       "0     0              0              0            0  \n",
       "1     0              0              0            0  \n",
       "2     0              0              0            0  \n",
       "3     0              0              1            0  \n",
       "4     0              0              1            0  \n",
       "5     0              0              1            0  \n",
       "6     0              0              1            0  \n",
       "7     0              0              1            0  \n",
       "8     0              0              1            0  \n",
       "9     0              0              1            0  \n",
       "\n",
       "[10 rows x 40 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df = pd.read_sql_table('DisasterResponse', engine)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'message', 'original', 'genre', 'related', 'request', 'offer',\n",
       "       'aid_related', 'medical_help', 'medical_products', 'search_and_rescue',\n",
       "       'security', 'military', 'water', 'food', 'shelter', 'clothing', 'money',\n",
       "       'missing_people', 'refugees', 'death', 'other_aid',\n",
       "       'infrastructure_related', 'transport', 'buildings', 'electricity',\n",
       "       'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
       "       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\n",
       "       'other_weather', 'direct_report', 'not_related'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the column features to be targeted\n",
    "target_columns = ['related', 'request', 'offer',\n",
    "       'aid_related', 'medical_help', 'medical_products', 'search_and_rescue',\n",
    "       'security', 'military', 'water', 'food', 'shelter', 'clothing', 'money',\n",
    "       'missing_people', 'refugees', 'death', 'other_aid',\n",
    "       'infrastructure_related', 'transport', 'buildings', 'electricity',\n",
    "       'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
    "       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\n",
    "       'other_weather', 'direct_report', 'not_related']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function to return the split of data to X and Y data arrays for training and testing\n",
    "def XY_values(df, X_columns, Y_columns):\n",
    "    X = df[X_columns].values\n",
    "    Y = df[Y_columns].values\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    '''\n",
    "    The tokenizer. The function in charge of processing text data, dividing and\n",
    "    analyzing it in each call. This function will clean the text from web page\n",
    "    addresses, it will split the text into word tokens, clean them from numbers\n",
    "    and quotation marks or other trademark symbols, classify them, clean them\n",
    "    from often words, and finally simplify the words to send a successful\n",
    "    result.\n",
    "\n",
    "    Function Parameters:\n",
    "\n",
    "        Required:\n",
    "\n",
    "            text : str ; the text to be tokenized.\n",
    "\n",
    "        Return:\n",
    "\n",
    "            clean_tokens : list of str ; the list of cleaned and treated words.\n",
    "    '''\n",
    "    # Changing every webpage for a space.\n",
    "    # With this regex we delete webpages with these characteristics:\n",
    "    #       1. http://www.name.ext or similar\n",
    "    #       2. http : www.name.ext or similar\n",
    "    #       3. http www.name.ext or similar\n",
    "    url_regex = 'http[s]?[\\s]?[:]?[\\s]?[\\/\\/]?(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    \n",
    "    # Then clean the texts of webpages addresses\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \" \")\n",
    "    \n",
    "    # Forgetting about the numbers and any non letter char\n",
    "    text = re.sub('[^a-zA-Z]',' ',text)\n",
    "    # Starting our bag of words.    \n",
    "    tokens = word_tokenize(text)\n",
    "    # Declaring the kind of tags will our lemmatizer work\n",
    "    tags = {\"J\": wordnet.ADJ,\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"R\": wordnet.ADV}\n",
    "    # Creating a list of words to be add to our stopwords list=total_stopwords,\n",
    "    # to eliminate them from our bag of words list. Use this list to improve our\n",
    "    # selection and have a cleaner results.\n",
    "    particular_words = ['kg'] \n",
    "    total_stopwords = particular_words + stopwords.words('english')\n",
    "    # Declaration of our lemmatizer and stemmer.\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # A loop to iterate in the list of words=tokens, for lemmitizing and\n",
    "    # stemming purposes. Adding the results to a new clean_tokens list.\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        # The lemmitizer will act depending of the tag of each word.\n",
    "        clean_tok = lemmatizer.lemmatize(tok, tags.get(pos_tag([tok])[0][1][0].upper(), wordnet.NOUN)).lower()\n",
    "        clean_tok = stemmer.stem(clean_tok)\n",
    "        if clean_tok not in total_stopwords:\n",
    "            clean_tokens.append(clean_tok)\n",
    "\n",
    "\n",
    "    return  clean_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'offic', 'tri', 'commiss', 'citi', 'oil', 'converg', 'woman', 'local', 'harcourt', 'tearga', 'conduct', 'southern', 'protest', 'port', 'elector', 'elect'}\n"
     ]
    }
   ],
   "source": [
    "# Let's test our tokenizer\n",
    "text='Some 2,000 women protesting against the conduct of the elections were teargassed as they tried to converge on the local electoral commission offices in the southern oil city of Port Harcourt.'\n",
    "set1 = set(tokenize(text))\n",
    "print(set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abandon', 'abit', 'abl', 'abroad', 'abroard', 'absolut', 'acacia', 'academi', 'access', 'account', 'accross', 'acra', 'across', 'activ', 'actual', 'address', 'ade', 'adj', 'adoken', 'adon', 'advanc', 'adventist', 'advic', 'advis', 'af', 'affect', 'afford', 'afka', 'afraid', 'afternoon', 'aftershak', 'aftershock', 'agent', 'aid', 'aidez', 'air', 'airdrop', 'airport', 'ajacdeb', 'akazya', 'alaza', 'albert', 'alert', 'alexandr', 'alimentari', 'aliv', 'alli', 'allot', 'allow', 'almost', 'along', 'alot', 'alreadi', 'alredi', 'also', 'altidor', 'alyan', 'amachu', 'ambroid', 'ambrois', 'amd', 'amerg', 'america', 'american', 'amiti', 'among', 'amount', 'angel', 'anglad', 'angri', 'ani', 'anim', 'ann', 'announc', 'anoth', 'ans', 'answer', 'anthoni', 'anti', 'antibiot', 'antoin', 'anymor', 'anyon', 'anyth', 'anything', 'anywher', 'aout', 'aplac', 'appar', 'appart', 'appear', 'apportez', 'appreci', 'approx', 'approxim', 'aprimatur', 'aquin', 'ar', 'aral', 'arcahai', 'archibishop', 'area', 'armi', 'around', 'arriv', 'artibonit', 'asap', 'asham', 'asi', 'ask', 'assiatnc', 'assici', 'assist', 'associ', 'assum', 'atlanta', 'attain', 'attent', 'au', 'aubran', 'aubri', 'aunt', 'aussi', 'author', 'authorithi', 'authoritiy', 'autor', 'avail', 'ave', 'avec', 'aven', 'avenu', 'aviat', 'avoid', 'avon', 'await', 'awasn', 'away', 'b', 'bab', 'babi', 'bac', 'back', 'bad', 'baddli', 'badli', 'bag', 'baie', 'bak', 'banbou', 'bandit', 'bank', 'banqu', 'bar', 'barad', 'bare', 'bari', 'bartholi', 'baryejo', 'base', 'basic', 'batail', 'batay', 'bateau', 'bateaux', 'bathroom', 'bato', 'batravil', 'batteri', 'beacaus', 'beach', 'bean', 'beauduy', 'becau', 'becaus', 'becom', 'becuas', 'becus', 'bed', 'befor', 'beforehand', 'begin', 'behalf', 'behind', 'bel', 'belanton', 'believ', 'belok', 'belong', 'benefit', 'benoit', 'benway', 'bernadett', 'bernard', 'bertholi', 'bertin', 'besid', 'besoin', 'best', 'betail', 'betem', 'betin', 'better', 'bi', 'big', 'billi', 'biotic', 'birth', 'bit', 'bizoton', 'blancha', 'blanket', 'bless', 'blind', 'block', 'blood', 'bo', 'bobi', 'boday', 'bodi', 'boi', 'bolo', 'boloss', 'bombardopoli', 'bon', 'bone', 'bonjour', 'bonrepo', 'book', 'boston', 'bosy', 'boten', 'bottl', 'boucan', 'boudon', 'bought', 'boukan', 'bouquet', 'bouquin', 'bourdon', 'bourri', 'bout', 'bowl', 'box', 'boyer', 'bph', 'brabal', 'brach', 'brachi', 'brenda', 'bring', 'bro', 'broadcast', 'broken', 'brother', 'brought', 'brown', 'broyer', 'bu', 'build', 'built', 'bunch', 'burn', 'busi', 'butt', 'buy', 'bwa', 'bye', 'c', 'cabaret', 'cafe', 'caffourfeil', 'cahnc', 'calboi', 'call', 'camp', 'campion', 'canada', 'canadien', 'canap', 'cannibal', 'canot', 'cant', 'cap', 'capit', 'car', 'caraib', 'caraibean', 'card', 'care', 'carefour', 'carfour', 'caribbean', 'carrefor', 'carrefour', 'case', 'castro', 'catastroph', 'catastrophi', 'cator', 'cattl', 'caus', 'cay', 'cdti', 'ce', 'cell', 'cellphon', 'cenor', 'censu', 'center', 'centr', 'central', 'certif', 'cesar', 'chada', 'champd', 'chanc', 'chansolm', 'char', 'charact', 'charg', 'charit', 'charl', 'chauvet', 'chavann', 'check', 'chees', 'chemin', 'chief', 'child', 'children', 'chili', 'chlidren', 'choos', 'choth', 'chretien', 'christ', 'christian', 'church', 'cistern', 'cite', 'citi', 'citizen', 'citron', 'citronni', 'civil', 'civilian', 'claud', 'claudel', 'clean', 'clear', 'clercin', 'clersin', 'client', 'climatec', 'close', 'closer', 'cloth', 'cloths', 'coast', 'coconut', 'cocteau', 'codada', 'coeur', 'coix', 'colaps', 'cold', 'coleg', 'collaps', 'colleg', 'collin', 'colombian', 'colon', 'come', 'comit', 'comite', 'comitt', 'comitte', 'commiss', 'committe', 'commun', 'communal', 'compani', 'complet', 'completli', 'complex', 'complic', 'comprehens', 'compromis', 'comput', 'comun', 'concept', 'concern', 'conclud', 'concret', 'condit', 'conduct', 'coner', 'confirm', 'congratul', 'consist', 'contact', 'continu', 'contribut', 'converg', 'convict', 'cook', 'cooki', 'cooper', 'coord', 'coordin', 'corn', 'corner', 'corp', 'corrosol', 'corver', 'cosquer', 'cost', 'cote', 'could', 'count', 'counti', 'countri', 'countrysid', 'cour', 'courag', 'cournoy', 'cours', 'court', 'courtyard', 'cousin', 'crack', 'crash', 'crazi', 'crematorium', 'creol', 'cri', 'crimbl', 'crimin', 'critic', 'croi', 'croix', 'cross', 'crossroad', 'crumbl', 'crush', 'cuba', 'cul', 'cup', 'current', 'cut', 'cutoff', 'cyber', 'dabon', 'dad', 'dal', 'damag', 'dame', 'damien', 'dampu', 'dang', 'dangerous', 'dant', 'date', 'day', 'ddp', 'de', 'dead', 'dear', 'death', 'debussi', 'debwo', 'decid', 'dehydr', 'delam', 'deland', 'delatalay', 'delivr', 'delma', 'demokr', 'demolish', 'demuni', 'deni', 'dep', 'depart', 'departemen', 'desalin', 'desir', 'desland', 'desmangl', 'desmond', 'desper', 'despit', 'dessalin', 'destitut', 'destri', 'destroy', 'detail', 'devast', 'develop', 'dew', 'dezam', 'diarrea', 'diarrhea', 'diaster', 'diden', 'didin', 'didnt', 'die', 'dieu', 'differ', 'difficult', 'difficulti', 'dificult', 'digi', 'digicel', 'dilig', 'dimanch', 'din', 'dincomb', 'diquini', 'dire', 'direct', 'directli', 'director', 'dirti', 'disappear', 'disast', 'discothequ', 'disposit', 'distribut', 'district', 'distroy', 'disturb', 'dlo', 'dnt', 'docteur', 'doctor', 'document', 'doesnt', 'dog', 'doi', 'dollar', 'domingu', 'dont', 'door', 'dorsainvil', 'doteur', 'downtown', 'drama', 'dress', 'drink', 'drinkabl', 'drive', 'driver', 'drop', 'droulement', 'drug', 'du', 'duboi', 'ducosqu', 'due', 'duga', 'dupont', 'dure', 'durinfg', 'duval', 'dyin', 'dza', 'earlier', 'earthquak', 'earthqual', 'eartquak', 'east', 'eat', 'eau', 'ebenez', 'ecol', 'eden', 'edh', 'effort', 'eglis', 'either', 'elderli', 'elect', 'elector', 'electr', 'electron', 'eleven', 'elim', 'els', 'embassi', 'emerg', 'emmen', 'empass', 'empti', 'end', 'energi', 'english', 'enough', 'enter', 'entir', 'entr', 'entranc', 'envoy', 'eo', 'equip', 'erathquak', 'erez', 'escap', 'espcial', 'especi', 'etc', 'european', 'evangeliqu', 'even', 'event', 'ever', 'everi', 'everybodi', 'everyon', 'everyth', 'everythig', 'everywher', 'evryth', 'exact', 'exampl', 'except', 'expans', 'expect', 'expens', 'expertis', 'extend', 'extens', 'eya', 'eye', 'fabr', 'face', 'factori', 'faculti', 'fair', 'fall', 'fame', 'famili', 'familli', 'famin', 'fampak', 'far', 'farther', 'fast', 'father', 'fatima', 'fed', 'feed', 'feel', 'feiull', 'felix', 'fell', 'felllow', 'femal', 'fermath', 'feuill', 'fever', 'field', 'fifth', 'fifti', 'fight', 'figur', 'figy', 'file', 'financi', 'find', 'fine', 'finish', 'fire', 'firefight', 'first', 'five', 'flat', 'fleur', 'floor', 'flore', 'flour', 'flu', 'flue', 'fm', 'foch', 'foget', 'fokal', 'follow', 'fond', 'fondat', 'fontamara', 'food', 'foof', 'foot', 'footbal', 'forc', 'forecast', 'foreign', 'foreman', 'forget', 'forgot', 'forgotten', 'fort', 'fotamara', 'fou', 'foul', 'found', 'foundat', 'four', 'fractur', 'franc', 'free', 'french', 'frere', 'friday', 'friend', 'froid', 'front', 'frontier', 'fthe', 'fueill', 'full', 'function', 'futur', 'ga', 'gabyon', 'galet', 'gallon', 'garantir', 'garden', 'gate', 'geffard', 'gen', 'gener', 'geographi', 'georg', 'georgia', 'gerald', 'gerard', 'geren', 'germ', 'get', 'girl', 'gist', 'give', 'givin', 'giyou', 'glass', 'gm', 'go', 'goat', 'goav', 'god', 'godfath', 'goldstar', 'golf', 'gommier', 'gon', 'gonaiv', 'gonav', 'gondol', 'gonzagu', 'good', 'goodmornig', 'got', 'gotten', 'goud', 'gourd', 'gov', 'govement', 'gover', 'govern', 'gp', 'grace', 'gran', 'grand', 'grant', 'gravel', 'great', 'gree', 'greedi', 'green', 'greet', 'gresey', 'gressier', 'gresssier', 'gresy', 'gro', 'ground', 'group', 'guard', 'guav', 'guerrier', 'guess', 'guibert', 'guilloux', 'guy', 'gve', 'gwav', 'hade', 'hait', 'haitel', 'haiti', 'haitian', 'haitien', 'haitienn', 'half', 'hand', 'handicap', 'handl', 'hangri', 'happen', 'happi', 'harcourt', 'hard', 'harder', 'harm', 'hasnt', 'haut', 'havent', 'havnt', 'head', 'health', 'hear', 'heard', 'heart', 'hecto', 'height', 'heklp', 'helicopt', 'hello', 'help', 'helpless', 'henn', 'herman', 'hi', 'high', 'highway', 'hill', 'hit', 'hlep', 'hod', 'hold', 'home', 'homeless', 'hometown', 'hone', 'honorat', 'hope', 'hopit', 'hospit', 'host', 'hot', 'hou', 'hous', 'household', 'hr', 'huge', 'human', 'humanitarian', 'humili', 'hunger', 'hungri', 'hurri', 'hurrican', 'hurt', 'husband', 'hv', 'hygien', 'hypertens', 'iar', 'ida', 'idea', 'ihav', 'ile', 'im', 'imakil', 'immacul', 'imp', 'impass', 'import', 'imposs', 'inasmo', 'includ', 'includind', 'incomplet', 'incomprehens', 'infect', 'info', 'infom', 'inform', 'infront', 'inhabit', 'injur', 'injuri', 'insecur', 'insid', 'inspir', 'instead', 'institut', 'intellig', 'intens', 'intent', 'intern', 'international', 'internationn', 'internet', 'interpert', 'interpret', 'intersect', 'inventori', 'invit', 'involv', 'ir', 'iroi', 'island', 'isnt', 'isol', 'issu', 'item', 'iti', 'itsmedicin', 'jack', 'jacmel', 'jacqu', 'jan', 'januari', 'janvier', 'jean', 'jeral', 'jeremi', 'jesu', 'jnauari', 'job', 'jode', 'john', 'join', 'joseph', 'journalist', 'juic', 'jumel', 'justic', 'juvenat', 'juvernat', 'k', 'ka', 'kabar', 'kachand', 'kan', 'kanal', 'kanapev', 'kanperen', 'kanpion', 'kapit', 'karaib', 'kastro', 'kay', 'keep', 'kenscoff', 'kenskof', 'kept', 'kf', 'kid', 'kill', 'kilomet', 'kind', 'king', 'kiskeya', 'kit', 'klecin', 'klesin', 'km', 'know', 'knowwhat', 'kokoy', 'kole', 'kolin', 'komin', 'kont', 'kot', 'kotex', 'krepsak', 'kwochi', 'l', 'la', 'label', 'laboul', 'lack', 'laferon', 'lakkoup', 'lakolin', 'lamantin', 'lamarr', 'lamartinier', 'lamentin', 'lamp', 'land', 'laplain', 'larg', 'lasil', 'last', 'latan', 'later', 'lathan', 'latrin', 'laval', 'law', 'lazarr', 'lazon', 'le', 'leader', 'learn', 'leas', 'least', 'leav', 'lecont', 'left', 'leg', 'lemak', 'lemoin', 'lemon', 'leogan', 'leogann', 'lespina', 'lespinass', 'let', 'letter', 'lettin', 'level', 'leyogan', 'liancourt', 'life', 'light', 'like', 'lilavoi', 'lilavwa', 'lilovoi', 'limi', 'linen', 'link', 'listen', 'littl', 'live', 'livelihood', 'livia', 'lizon', 'lobod', 'local', 'locat', 'logan', 'loger', 'lompr', 'long', 'longer', 'look', 'lord', 'lose', 'loss', 'lost', 'lot', 'louben', 'loui', 'lovat', 'low', 'lucaman', 'lucien', 'lumier', 'lwe', 'lyce', 'lypedha', 'macajoux', 'made', 'magi', 'magloir', 'magnitud', 'magua', 'mahoti', 'mai', 'main', 'mair', 'makadi', 'makaj', 'make', 'mal', 'malgro', 'malik', 'mamag', 'man', 'mang', 'mangones', 'mani', 'maniga', 'map', 'mapou', 'mar', 'marc', 'marchand', 'maren', 'margot', 'mariani', 'mariela', 'mariella', 'marin', 'mark', 'market', 'marotier', 'martin', 'martisan', 'martiss', 'martissan', 'maryani', 'mask', 'masson', 'mathieu', 'matisan', 'matissan', 'mattress', 'maty', 'mawouj', 'maximum', 'mayb', 'mean', 'meanwhil', 'mechan', 'medecin', 'mediac', 'medic', 'medica', 'medicaton', 'medicin', 'medilien', 'medium', 'meet', 'member', 'men', 'meno', 'merchand', 'merci', 'mercier', 'mercredi', 'mere', 'merg', 'merger', 'mesag', 'messag', 'metal', 'mgr', 'miami', 'mich', 'michel', 'midnight', 'midway', 'might', 'miki', 'militari', 'milk', 'mind', 'mine', 'minustah', 'minut', 'miracl', 'miragoan', 'miseri', 'miss', 'mission', 'mitchel', 'mix', 'mobil', 'moghav', 'moinret', 'mois', 'mole', 'moleard', 'moleya', 'mom', 'moment', 'mon', 'monarqu', 'monbin', 'mond', 'money', 'monpely', 'monseigneur', 'mont', 'month', 'montroui', 'moquett', 'moral', 'morin', 'mormon', 'morn', 'mornin', 'mostli', 'mother', 'motocycl', 'mounn', 'mountain', 'move', 'moya', 'ms', 'msg', 'much', 'municip', 'musso', 'must', 'n', 'na', 'nabril', 'nam', 'name', 'nan', 'nation', 'nativ', 'natur', 'nazon', 'nd', 'near', 'nearbi', 'necess', 'nee', 'need', 'neglect', 'neighbor', 'neighborhood', 'neighbour', 'neighbourhood', 'neither', 'nephew', 'nervou', 'network', 'neurologist', 'never', 'new', 'news', 'next', 'nichola', 'nicola', 'night', 'nipp', 'nirvana', 'nite', 'noaill', 'nobodi', 'noel', 'noon', 'normal', 'north', 'northern', 'note', 'noth', 'notr', 'nou', 'noulagu', 'nourritur', 'nowher', 'number', 'obri', 'obscur', 'occupi', 'occur', 'ocean', 'ode', 'odm', 'odor', 'offer', 'offic', 'offici', 'oh', 'oil', 'ok', 'okay', 'okt', 'old', 'ona', 'one', 'oner', 'onli', 'ont', 'open', 'openin', 'openli', 'openstreetmap', 'oper', 'opersonnel', 'opportun', 'opposit', 'oragnis', 'orchide', 'order', 'organ', 'organis', 'origin', 'oth', 'ou', 'ourselv', 'outsid', 'overcrowd', 'oversea', 'overwhelm', 'owner', 'p', 'pa', 'pack', 'paco', 'pad', 'pager', 'pain', 'painson', 'paix', 'pak', 'pal', 'palac', 'palai', 'paloma', 'pam', 'pami', 'pampak', 'panamericain', 'panamerican', 'panel', 'pap', 'paper', 'parachut', 'parent', 'pari', 'parish', 'parisien', 'park', 'part', 'particularli', 'pascal', 'pass', 'passport', 'past', 'pastor', 'patient', 'pax', 'pay', 'pea', 'peapl', 'peitit', 'pelerin', 'peopl', 'pepl', 'per', 'period', 'perish', 'pernier', 'perou', 'perpetuel', 'perrier', 'person', 'perticular', 'perticularli', 'peruvian', 'pestel', 'peter', 'petion', 'petionvil', 'petit', 'pewoden', 'philadelphia', 'philanthrop', 'philipp', 'phone', 'physic', 'pi', 'pick', 'pictur', 'pie', 'pierr', 'pig', 'pile', 'pinson', 'pistach', 'piti', 'pitre', 'pitrea', 'pl', 'place', 'plage', 'plain', 'plaisanc', 'plaj', 'plan', 'plant', 'plantat', 'plastic', 'plastik', 'plate', 'plaza', 'plea', 'plead', 'pleas', 'plein', 'plenn', 'plu', 'pm', 'pnud', 'point', 'polic', 'polish', 'ponsond', 'pont', 'poop', 'poor', 'pop', 'popul', 'populair', 'port', 'portail', 'posit', 'possibl', 'pot', 'potabl', 'pou', 'pound', 'poupla', 'pour', 'power', 'pre', 'precaut', 'precis', 'predict', 'pregnant', 'premier', 'premy', 'present', 'presid', 'presidenti', 'pressur', 'price', 'priest', 'primati', 'princ', 'prison', 'probabl', 'problem', 'process', 'product', 'professor', 'proffession', 'prolong', 'prolonge', 'prolongu', 'prosi', 'prospect', 'prosper', 'protect', 'protest', 'provid', 'provinc', 'provinci', 'psycholog', 'psychologist', 'public', 'put', 'pver', 'pylon', 'pyron', 'quak', 'que', 'qui', 'quickli', 'quiet', 'quikli', 'r', 'rabel', 'raboto', 'radio', 'rafadek', 'rail', 'rain', 'rainni', 'rais', 'ranch', 'ray', 'rd', 'reach', 'read', 'readi', 'realli', 'reason', 'rebuild', 'receiv', 'recent', 'reciev', 'reciv', 'red', 'refil', 'refug', 'refuge', 'regar', 'regard', 'region', 'registri', 'relay', 'relief', 'remain', 'rememb', 'remov', 'renaiss', 'rent', 'reoccurr', 'rep', 'repair', 'repay', 'replac', 'repli', 'repo', 'repons', 'report', 'request', 'rescu', 'reserv', 'residenti', 'respect', 'respond', 'respons', 'respos', 'ressourc', 'rest', 'restaur', 'resum', 'return', 'rheto', 'rhetor', 'ri', 'ribonci', 'rice', 'richard', 'ridor', 'righout', 'right', 'river', 'rivier', 'rivy', 'riy', 'road', 'rob', 'roch', 'rock', 'roi', 'roof', 'room', 'roug', 'rout', 'royal', 'rservoir', 'rte', 'rubbl', 'ruben', 'rue', 'ruell', 'rumor', 'run', 'rural', 'sabouc', 'sac', 'sacr', 'sad', 'safe', 'safeti', 'said', 'saint', 'salem', 'salut', 'salvag', 'santo', 'sapotil', 'sart', 'sat', 'sath', 'saturday', 'savan', 'savann', 'savanna', 'save', 'say', 'scale', 'scar', 'school', 'scienc', 'scream', 'sea', 'season', 'sebastien', 'second', 'secour', 'section', 'secton', 'secur', 'securit', 'see', 'seed', 'seek', 'seem', 'seguineau', 'seismic', 'seksyon', 'self', 'sell', 'seminarist', 'sen', 'send', 'seneg', 'sengor', 'sens', 'sent', 'sentenc', 'seriou', 'serv', 'servic', 'session', 'set', 'sever', 'seza', 'shada', 'shake', 'share', 'sheet', 'shelet', 'shelter', 'shock', 'shoe', 'short', 'shortag', 'show', 'shower', 'sibert', 'sick', 'sicot', 'sid', 'side', 'signal', 'signo', 'siko', 'silenc', 'silent', 'silo', 'simpl', 'sinc', 'sirin', 'sise', 'sister', 'site', 'sitron', 'situat', 'sleep', 'sleepo', 'slelak', 'slum', 'sm', 'small', 'smell', 'smelli', 'soap', 'social', 'sociologist', 'soeur', 'sogebank', 'sogesol', 'soil', 'solar', 'solei', 'soleil', 'soley', 'solidarit', 'somebodi', 'someon', 'somet', 'someth', 'somewher', 'somm', 'son', 'sond', 'soon', 'sorbonn', 'sorri', 'sot', 'sound', 'sourc', 'sourthern', 'south', 'southeast', 'southern', 'souverin', 'space', 'spaghetti', 'speak', 'special', 'specialist', 'specif', 'spous', 'squar', 'st', 'sta', 'staff', 'star', 'start', 'starv', 'starvat', 'starvin', 'state', 'station', 'stay', 'still', 'stomach', 'stop', 'storm', 'straight', 'stre', 'streer', 'street', 'strenght', 'strong', 'struck', 'structur', 'struggl', 'stuck', 'student', 'studi', 'studio', 'stuff', 'suburb', 'suffer', 'sugar', 'sugarcan', 'suli', 'sun', 'sunburn', 'suppli', 'support', 'suppos', 'sur', 'sure', 'surgeri', 'surin', 'surviv', 'survivor', 'suzann', 'svp', 'sylvio', 'symbol', 'ta', 'tabarr', 'taifer', 'take', 'talk', 'tampon', 'tant', 'teacher', 'team', 'tearga', 'technician', 'teh', 'telephon', 'televis', 'tell', 'telphon', 'temporari', 'ten', 'tent', 'term', 'terr', 'terribl', 'tete', 'text', 'th', 'thak', 'thamk', 'thank', 'thankyou', 'thee', 'themselv', 'ther', 'therea', 'theres', 'thi', 'thief', 'thing', 'think', 'third', 'thirst', 'thirsti', 'thirteen', 'thn', 'thomassin', 'though', 'thought', 'thousand', 'thursday', 'thursti', 'ti', 'tiburon', 'till', 'time', 'tinant', 'tire', 'tirivi', 'titu', 'tn', 'tnh', 'today', 'togeth', 'toilet', 'toiletri', 'toilett', 'told', 'toll', 'tomasin', 'tomato', 'tomorrow', 'tonight', 'tool', 'toour', 'top', 'torbec', 'torbeck', 'tortu', 'total', 'tou', 'touch', 'tout', 'toward', 'town', 'track', 'tractor', 'tragedi', 'train', 'tranfer', 'tranqu', 'tranquil', 'transfer', 'translat', 'transport', 'trap', 'trash', 'traumat', 'treat', 'treatment', 'tree', 'tremor', 'tri', 'trinit', 'troubl', 'truck', 'true', 'truncat', 'tue', 'tuesday', 'tun', 'turb', 'turgeau', 'turn', 'twelv', 'two', 'type', 'u', 'uebh', 'ulcer', 'umpass', 'un', 'unabl', 'unclear', 'underneath', 'underprivileg', 'understand', 'underwear', 'unemploy', 'unibank', 'uniqu', 'unit', 'univers', 'universit', 'unless', 'unrecogn', 'unrest', 'upon', 'upper', 'urgent', 'urjent', 'us', 'usa', 'use', 'usecondari', 'v', 'valley', 'vencent', 'vendor', 'venez', 'veret', 'verett', 'veri', 'verifi', 'verna', 'vert', 'via', 'vice', 'victim', 'victori', 'vien', 'vifranc', 'vila', 'vile', 'vill', 'villag', 'violenc', 'visa', 'visit', 'vitamin', 'vitim', 'viuctim', 'vivi', 'vob', 'voic', 'volunt', 'w', 'wa', 'wait', 'walk', 'wall', 'waney', 'want', 'warm', 'washington', 'wat', 'water', 'watson', 'wave', 'way', 'wayn', 'wc', 'wear', 'wearingg', 'wednesday', 'week', 'weekend', 'well', 'wesley', 'west', 'wet', 'wether', 'whatev', 'whatsoev', 'wherev', 'whether', 'whi', 'whichev', 'white', 'whole', 'whose', 'wi', 'wife', 'wiht', 'willi', 'williamson', 'wipe', 'wish', 'wit', 'without', 'witout', 'woman', 'women', 'wonder', 'word', 'work', 'worker', 'world', 'woul', 'would', 'wound', 'wow', 'write', 'writing', 'wrong', 'yard', 'ye', 'year', 'yearthquak', 'yesterday', 'yet', 'young', 'youth', 'z', 'zepol', 'zinc', 'zone'] 1990\n"
     ]
    }
   ],
   "source": [
    "# To see more results and analize them, let's check the first 1000 rows.\n",
    "# We can see that there are a lot of words without meaning that we can clean and improve our algorithm.\n",
    "# No for the moment.\n",
    "for a in range(1000):\n",
    "    set1.update(tokenize(df.message.iloc[a]))\n",
    "print(sorted(set1), len(set1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the ML algorithm which we will test with this dataset.\n",
    "def Random_Forest_pipeline():\n",
    "    return Pipeline([\n",
    "                    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "                    ])\n",
    "\n",
    "def Logistic_Regression_pipeline():\n",
    "    return Pipeline([\n",
    "                    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultiOutputClassifier(LogisticRegression()))\n",
    "                    ])\n",
    "\n",
    "def Decision_Tree_pipeline():\n",
    "    return Pipeline([\n",
    "                    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultiOutputClassifier(DecisionTreeClassifier()))\n",
    "                    ])\n",
    "\n",
    "def GradientBoostingClassifier():\n",
    "    return Pipeline([\n",
    "                    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultiOutputClassifier(GradientBoostingClassifier(max_depth=6)))\n",
    "                    ])\n",
    "\n",
    "def SVC():\n",
    "    return Pipeline([\n",
    "                    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf', MultiOutputClassifier(SVC()))\n",
    "                    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23916, 40)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some checks to our data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>...</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "      <th>not_related</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "      <td>23916.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15923.868749</td>\n",
       "      <td>0.897725</td>\n",
       "      <td>0.231477</td>\n",
       "      <td>0.009617</td>\n",
       "      <td>0.630707</td>\n",
       "      <td>0.123223</td>\n",
       "      <td>0.084755</td>\n",
       "      <td>0.071333</td>\n",
       "      <td>0.048796</td>\n",
       "      <td>0.075305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071709</td>\n",
       "      <td>0.416290</td>\n",
       "      <td>0.131586</td>\n",
       "      <td>0.132505</td>\n",
       "      <td>0.027973</td>\n",
       "      <td>0.131502</td>\n",
       "      <td>0.048587</td>\n",
       "      <td>0.078818</td>\n",
       "      <td>0.260202</td>\n",
       "      <td>0.102275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8777.451220</td>\n",
       "      <td>0.303015</td>\n",
       "      <td>0.421785</td>\n",
       "      <td>0.097596</td>\n",
       "      <td>0.482623</td>\n",
       "      <td>0.328700</td>\n",
       "      <td>0.278523</td>\n",
       "      <td>0.257386</td>\n",
       "      <td>0.215445</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258011</td>\n",
       "      <td>0.492953</td>\n",
       "      <td>0.338047</td>\n",
       "      <td>0.339047</td>\n",
       "      <td>0.164899</td>\n",
       "      <td>0.337956</td>\n",
       "      <td>0.215007</td>\n",
       "      <td>0.269459</td>\n",
       "      <td>0.438754</td>\n",
       "      <td>0.303015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16830.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23334.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>30264.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id       related       request         offer   aid_related  \\\n",
       "count  23916.000000  23916.000000  23916.000000  23916.000000  23916.000000   \n",
       "mean   15923.868749      0.897725      0.231477      0.009617      0.630707   \n",
       "std     8777.451220      0.303015      0.421785      0.097596      0.482623   \n",
       "min        7.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%     8266.000000      1.000000      0.000000      0.000000      0.000000   \n",
       "50%    16830.500000      1.000000      0.000000      0.000000      1.000000   \n",
       "75%    23334.250000      1.000000      0.000000      0.000000      1.000000   \n",
       "max    30264.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       medical_help  medical_products  search_and_rescue      security  \\\n",
       "count  23916.000000      23916.000000       23916.000000  23916.000000   \n",
       "mean       0.123223          0.084755           0.071333      0.048796   \n",
       "std        0.328700          0.278523           0.257386      0.215445   \n",
       "min        0.000000          0.000000           0.000000      0.000000   \n",
       "25%        0.000000          0.000000           0.000000      0.000000   \n",
       "50%        0.000000          0.000000           0.000000      0.000000   \n",
       "75%        0.000000          0.000000           0.000000      0.000000   \n",
       "max        1.000000          1.000000           1.000000      1.000000   \n",
       "\n",
       "           military      ...       other_infrastructure  weather_related  \\\n",
       "count  23916.000000      ...               23916.000000     23916.000000   \n",
       "mean       0.075305      ...                   0.071709         0.416290   \n",
       "std        0.263889      ...                   0.258011         0.492953   \n",
       "min        0.000000      ...                   0.000000         0.000000   \n",
       "25%        0.000000      ...                   0.000000         0.000000   \n",
       "50%        0.000000      ...                   0.000000         0.000000   \n",
       "75%        0.000000      ...                   0.000000         1.000000   \n",
       "max        1.000000      ...                   1.000000         1.000000   \n",
       "\n",
       "             floods         storm          fire    earthquake          cold  \\\n",
       "count  23916.000000  23916.000000  23916.000000  23916.000000  23916.000000   \n",
       "mean       0.131586      0.132505      0.027973      0.131502      0.048587   \n",
       "std        0.338047      0.339047      0.164899      0.337956      0.215007   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       other_weather  direct_report   not_related  \n",
       "count   23916.000000   23916.000000  23916.000000  \n",
       "mean        0.078818       0.260202      0.102275  \n",
       "std         0.269459       0.438754      0.303015  \n",
       "min         0.000000       0.000000      0.000000  \n",
       "25%         0.000000       0.000000      0.000000  \n",
       "50%         0.000000       0.000000      0.000000  \n",
       "75%         0.000000       1.000000      0.000000  \n",
       "max         1.000000       1.000000      1.000000  \n",
       "\n",
       "[8 rows x 37 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More checks to remember it\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I tried with all the algorithms before. After some errors, slow executions, and different accuracies results,\n",
    "# i decide to go simpler with Random_Forest, because it gave me a respectfully result and less time than others.\n",
    "# My computer is not to fast to help check more in detail every algorithm.\n",
    "pipeline = Random_Forest_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's split the data for training purposes\n",
    "X, Y = XY_values(df, 'message', target_columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "# In my exercises i took the time for further decisions.\n",
    "start = time.perf_counter()\n",
    "# Let's train\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 238.297827245\n",
      "0.33131270903\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.92      0.98      0.95      4292\n",
      "               request       0.88      0.73      0.80      1099\n",
      "                 offer       0.96      0.51      0.67        51\n",
      "           aid_related       0.84      0.90      0.87      3023\n",
      "          medical_help       0.88      0.54      0.67       602\n",
      "      medical_products       0.95      0.58      0.72       423\n",
      "     search_and_rescue       0.94      0.76      0.84       361\n",
      "              security       0.96      0.86      0.90       229\n",
      "              military       0.91      0.79      0.85       345\n",
      "                 water       0.91      0.61      0.73       473\n",
      "                  food       0.91      0.71      0.80       748\n",
      "               shelter       0.94      0.70      0.80       669\n",
      "              clothing       0.97      0.79      0.87       169\n",
      "                 money       0.96      0.75      0.84       238\n",
      "        missing_people       0.97      0.83      0.89       168\n",
      "              refugees       0.95      0.81      0.87       386\n",
      "                 death       0.93      0.60      0.73       349\n",
      "             other_aid       0.90      0.42      0.57       920\n",
      "infrastructure_related       0.95      0.56      0.70       641\n",
      "             transport       0.94      0.61      0.74       394\n",
      "             buildings       0.94      0.58      0.72       382\n",
      "           electricity       0.97      0.81      0.88       218\n",
      "                 tools       0.98      0.76      0.86        84\n",
      "             hospitals       0.97      0.71      0.82       143\n",
      "                 shops       0.98      0.71      0.82        65\n",
      "           aid_centers       0.98      0.73      0.84       162\n",
      "  other_infrastructure       0.93      0.45      0.60       364\n",
      "       weather_related       0.91      0.86      0.88      1970\n",
      "                floods       0.97      0.71      0.82       647\n",
      "                 storm       0.88      0.69      0.78       609\n",
      "                  fire       0.97      0.74      0.84       141\n",
      "            earthquake       0.93      0.83      0.88       615\n",
      "                  cold       0.94      0.79      0.86       231\n",
      "         other_weather       0.88      0.46      0.60       366\n",
      "         direct_report       0.84      0.61      0.71      1240\n",
      "           not_related       0.64      0.19      0.29       492\n",
      "\n",
      "           avg / total       0.90      0.75      0.81     23309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's the predict with the testing data and our trained model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "# Results\n",
    "print('time:', time.perf_counter()-start)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=target_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7fa1a9f007b8>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7fa1a9f007b8>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'clf__estimator__n_jobs': 1,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check the parameters we have for our model and which can we try to change to find better results with a\n",
    "# GridSearch.\n",
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the parameters to iterate in a Grid Search\n",
    "# This is our options, but the reality is that i coudn't access good GPU's to make it fast.\n",
    "# In the practice i only could make it iterating two parameters.\n",
    "# Then i made some Grid Search with some of these parameters, several times.\n",
    "# Even that i found interesting results for my next model implementation further.\n",
    "parameters = {\n",
    "             'vect__max_df': (0.75, 1.0),\n",
    "             'vect__max_features': (None, 10000),\n",
    "             'tfidf__norm': ('l2','l1'),\n",
    "             'tfidf__use_idf': (True, False),\n",
    "             'clf__estimator__criterion': ['gini','entropy'],\n",
    "             'clf__estimator__n_estimators': [10,250],\n",
    "             'clf__estimator__random_state': [42, 69]\n",
    "            }\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6895.592182282\n",
      "0.456939799331\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.91      0.99      0.95      4292\n",
      "               request       0.90      0.79      0.84      1099\n",
      "                 offer       1.00      0.71      0.83        51\n",
      "           aid_related       0.83      0.94      0.88      3023\n",
      "          medical_help       0.88      0.61      0.72       602\n",
      "      medical_products       0.96      0.63      0.76       423\n",
      "     search_and_rescue       0.94      0.88      0.91       361\n",
      "              security       0.98      0.90      0.94       229\n",
      "              military       0.94      0.83      0.88       345\n",
      "                 water       0.92      0.77      0.84       473\n",
      "                  food       0.92      0.87      0.89       748\n",
      "               shelter       0.94      0.78      0.85       669\n",
      "              clothing       0.97      0.87      0.92       169\n",
      "                 money       0.97      0.82      0.89       238\n",
      "        missing_people       0.99      0.91      0.95       168\n",
      "              refugees       0.96      0.89      0.92       386\n",
      "                 death       0.95      0.69      0.80       349\n",
      "             other_aid       0.94      0.45      0.61       920\n",
      "infrastructure_related       0.98      0.62      0.76       641\n",
      "             transport       0.94      0.66      0.78       394\n",
      "             buildings       0.93      0.63      0.75       382\n",
      "           electricity       0.96      0.86      0.91       218\n",
      "                 tools       1.00      0.81      0.89        84\n",
      "             hospitals       0.97      0.80      0.88       143\n",
      "                 shops       0.98      0.75      0.85        65\n",
      "           aid_centers       1.00      0.83      0.91       162\n",
      "  other_infrastructure       0.99      0.52      0.68       364\n",
      "       weather_related       0.92      0.91      0.91      1970\n",
      "                floods       0.98      0.78      0.87       647\n",
      "                 storm       0.87      0.81      0.84       609\n",
      "                  fire       0.97      0.83      0.89       141\n",
      "            earthquake       0.94      0.91      0.92       615\n",
      "                  cold       0.96      0.86      0.90       231\n",
      "         other_weather       0.91      0.52      0.66       366\n",
      "         direct_report       0.87      0.68      0.77      1240\n",
      "           not_related       0.72      0.21      0.32       492\n",
      "\n",
      "           avg / total       0.91      0.81      0.85     23309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same process to traint the model and check it with the testing data\n",
    "start = time.perf_counter()\n",
    "cv.fit(X_train, y_train)\n",
    "y_pred = cv.predict(X_test)\n",
    "print('time:', time.perf_counter()-start)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=target_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([  129.94203766,  1076.02259247]), 'std_fit_time': array([ 0.89403854,  7.38743023]), 'mean_score_time': array([ 47.55205997,  92.78247309]), 'std_score_time': array([ 0.94339057,  0.25359839]), 'param_clf__estimator__criterion': masked_array(data = ['gini' 'gini'],\n",
      "             mask = [False False],\n",
      "       fill_value = ?)\n",
      ", 'param_clf__estimator__n_estimators': masked_array(data = [10 250],\n",
      "             mask = [False False],\n",
      "       fill_value = ?)\n",
      ", 'params': [{'clf__estimator__criterion': 'gini', 'clf__estimator__n_estimators': 10}, {'clf__estimator__criterion': 'gini', 'clf__estimator__n_estimators': 250}], 'split0_test_score': array([ 0.25901537,  0.36986516]), 'split1_test_score': array([ 0.25560608,  0.37180492]), 'split2_test_score': array([ 0.25043124,  0.36176886]), 'mean_test_score': array([ 0.25501777,  0.36781309]), 'std_test_score': array([ 0.0035291,  0.0043465]), 'rank_test_score': array([2, 1], dtype=int32), 'split0_train_score': array([ 0.83526737,  0.99662851]), 'split1_train_score': array([ 0.83543708,  0.99741278]), 'split2_train_score': array([ 0.84139553,  0.99811838]), 'mean_train_score': array([ 0.83736666,  0.99738656]), 'std_train_score': array([ 0.00284968,  0.00060852])}\n"
     ]
    }
   ],
   "source": [
    "# A way to see the parameters involved in the last training.\n",
    "print(cv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For improving our model we want to try to add some feature.\n",
    "# The first one, is check if there is any organization's name on any text.\n",
    "# First let's see what we can have in the practice, with this function:\n",
    "def checking_orgs(text, a):\n",
    "        # tokenize words and remove regular words.\n",
    "        words = word_tokenize(text)\n",
    "        words = [w for w in words if w.lower() not in stopwords.words('english')]\n",
    "        # Tag the words\n",
    "        ptree = pos_tag(words)\n",
    "        # With ne_chunk utility checking the tree for organization's names\n",
    "        for w in tree2conlltags(ne_chunk(ptree)):\n",
    "            if (w[2][2:] == 'ORGANIZATION') and (w[1] == 'NNP'):\n",
    "                print(a, w)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ('St.', 'NNP', 'I-ORGANIZATION')\n",
      "1 ('Croix', 'NNP', 'I-ORGANIZATION')\n",
      "8 ('Comitee', 'NNP', 'B-ORGANIZATION')\n",
      "15 ('ASAP', 'NNP', 'B-ORGANIZATION')\n",
      "15 ('Come', 'NNP', 'B-ORGANIZATION')\n",
      "23 ('Comite', 'NNP', 'B-ORGANIZATION')\n",
      "23 ('Miracle', 'NNP', 'I-ORGANIZATION')\n",
      "29 ('ASAP', 'NNP', 'B-ORGANIZATION')\n",
      "39 ('SOS', 'NNP', 'B-ORGANIZATION')\n",
      "48 ('Bernadette', 'NNP', 'B-ORGANIZATION')\n",
      "59 ('ADJS', 'NNP', 'B-ORGANIZATION')\n",
      "73 ('FIRE', 'NNP', 'B-ORGANIZATION')\n",
      "89 ('Santo', 'NNP', 'B-ORGANIZATION')\n",
      "95 ('ONA', 'NNP', 'B-ORGANIZATION')\n",
      "97 ('Good', 'NNP', 'B-ORGANIZATION')\n",
      "99 ('EDH', 'NNP', 'B-ORGANIZATION')\n",
      "99 ('Electricity', 'NNP', 'B-ORGANIZATION')\n",
      "99 ('Haiti', 'NNP', 'I-ORGANIZATION')\n",
      "127 ('Hospital', 'NNP', 'B-ORGANIZATION')\n",
      "128 ('Delmas', 'NNP', 'B-ORGANIZATION')\n",
      "138 ('USA', 'NNP', 'B-ORGANIZATION')\n",
      "138 ('Im', 'NNP', 'I-ORGANIZATION')\n",
      "151 ('DDP', 'NNP', 'B-ORGANIZATION')\n",
      "151 ('TOrbeck', 'NNP', 'B-ORGANIZATION')\n",
      "152 ('GPS', 'NNP', 'B-ORGANIZATION')\n",
      "153 ('DLO', 'NNP', 'B-ORGANIZATION')\n",
      "154 ('Nicholas', 'NNP', 'B-ORGANIZATION')\n",
      "169 ('NEED', 'NNP', 'B-ORGANIZATION')\n",
      "181 ('Gonaives', 'NNP', 'B-ORGANIZATION')\n",
      "184 ('THN', 'NNP', 'B-ORGANIZATION')\n",
      "185 ('National', 'NNP', 'B-ORGANIZATION')\n",
      "185 ('Television', 'NNP', 'I-ORGANIZATION')\n",
      "191 ('Pot', 'NNP', 'B-ORGANIZATION')\n",
      "198 ('Translator', 'NNP', 'B-ORGANIZATION')\n",
      "210 ('SIKO', 'NNP', 'B-ORGANIZATION')\n",
      "214 ('HELP', 'NNP', 'B-ORGANIZATION')\n",
      "214 ('HANDICAP', 'NNP', 'B-ORGANIZATION')\n",
      "219 ('TNH', 'NNP', 'B-ORGANIZATION')\n",
      "223 ('KOMIN', 'NNP', 'B-ORGANIZATION')\n",
      "223 ('VICTIMS', 'NNP', 'B-ORGANIZATION')\n",
      "223 ('LOST', 'NNP', 'B-ORGANIZATION')\n",
      "223 ('LOST', 'NNP', 'B-ORGANIZATION')\n",
      "223 ('PEOPLE', 'NNP', 'B-ORGANIZATION')\n",
      "223 ('MONBIN', 'NNP', 'B-ORGANIZATION')\n",
      "229 ('HELP', 'NNP', 'B-ORGANIZATION')\n",
      "229 ('CENTER', 'NNP', 'B-ORGANIZATION')\n",
      "229 ('DAMIEN', 'NNP', 'B-ORGANIZATION')\n",
      "232 ('HELP', 'NNP', 'B-ORGANIZATION')\n",
      "232 ('PLEASE', 'NNP', 'B-ORGANIZATION')\n",
      "232 ('YARD', 'NNP', 'B-ORGANIZATION')\n",
      "238 ('PETION', 'NNP', 'B-ORGANIZATION')\n",
      "238 ('IMPASS', 'NNP', 'B-ORGANIZATION')\n",
      "239 ('MARTISAN', 'NNP', 'B-ORGANIZATION')\n",
      "241 ('PEWODEN', 'NNP', 'B-ORGANIZATION')\n",
      "241 ('FIFTH', 'NNP', 'B-ORGANIZATION')\n",
      "241 ('LIKE', 'NNP', 'B-ORGANIZATION')\n",
      "243 ('TN', 'NNP', 'B-ORGANIZATION')\n",
      "243 ('New', 'NNP', 'I-ORGANIZATION')\n",
      "249 ('MORNIN', 'NNP', 'B-ORGANIZATION')\n",
      "249 ('CARD', 'NNP', 'B-ORGANIZATION')\n",
      "253 ('EDH', 'NNP', 'B-ORGANIZATION')\n",
      "253 ('PLEASE', 'NNP', 'B-ORGANIZATION')\n",
      "253 ('DIE', 'NNP', 'B-ORGANIZATION')\n",
      "254 ('Siko', 'NNP', 'B-ORGANIZATION')\n",
      "255 ('STREET', 'NNP', 'B-ORGANIZATION')\n",
      "255 ('HAITIEN', 'NNP', 'B-ORGANIZATION')\n",
      "255 ('WISH', 'NNP', 'B-ORGANIZATION')\n",
      "255 ('ELECTRIC', 'NNP', 'B-ORGANIZATION')\n",
      "255 ('ROAD', 'NNP', 'B-ORGANIZATION')\n",
      "258 ('NEED', 'NNP', 'B-ORGANIZATION')\n",
      "258 ('MEDICINE', 'NNP', 'B-ORGANIZATION')\n",
      "261 ('EYA', 'NNP', 'B-ORGANIZATION')\n",
      "262 ('NEED', 'NNP', 'B-ORGANIZATION')\n",
      "262 ('HELP', 'NNP', 'B-ORGANIZATION')\n",
      "265 ('N', 'NNP', 'B-ORGANIZATION')\n",
      "284 ('Street', 'NNP', 'B-ORGANIZATION')\n",
      "285 ('Santo', 'NNP', 'B-ORGANIZATION')\n",
      "292 ('Rue', 'NNP', 'B-ORGANIZATION')\n",
      "308 ('MADE', 'NNP', 'B-ORGANIZATION')\n",
      "308 ('MAP', 'NNP', 'B-ORGANIZATION')\n",
      "311 ('DIGICEL', 'NNP', 'B-ORGANIZATION')\n",
      "311 ('LEOGANE', 'NNP', 'B-ORGANIZATION')\n",
      "314 ('Restaurant', 'NNP', 'B-ORGANIZATION')\n",
      "314 ('Rue', 'NNP', 'I-ORGANIZATION')\n",
      "314 ('Du', 'NNP', 'I-ORGANIZATION')\n",
      "314 ('Centre', 'NNP', 'I-ORGANIZATION')\n",
      "325 ('Fort', 'NNP', 'B-ORGANIZATION')\n",
      "325 ('Jacques', 'NNP', 'I-ORGANIZATION')\n",
      "325 ('Fermathe', 'NNP', 'B-ORGANIZATION')\n",
      "329 ('Immacule', 'NNP', 'B-ORGANIZATION')\n",
      "329 ('Conception', 'NNP', 'I-ORGANIZATION')\n",
      "329 ('Hospital', 'NNP', 'I-ORGANIZATION')\n",
      "349 ('HERMANE', 'NNP', 'B-ORGANIZATION')\n",
      "349 ('PASSPORT', 'NNP', 'B-ORGANIZATION')\n",
      "349 ('TELL', 'NNP', 'B-ORGANIZATION')\n",
      "352 ('Good', 'NNP', 'B-ORGANIZATION')\n",
      "353 ('Gonaives', 'NNP', 'B-ORGANIZATION')\n",
      "354 ('NAZON', 'NNP', 'B-ORGANIZATION')\n",
      "354 ('LOGAN', 'NNP', 'B-ORGANIZATION')\n",
      "357 ('CITY', 'NNP', 'B-ORGANIZATION')\n",
      "357 ('WANTED', 'NNP', 'B-ORGANIZATION')\n",
      "357 ('WATERS', 'NNP', 'B-ORGANIZATION')\n",
      "357 ('HOUSES', 'NNP', 'B-ORGANIZATION')\n",
      "357 ('BEACAUSE', 'NNP', 'B-ORGANIZATION')\n",
      "357 ('HOUSES', 'NNP', 'B-ORGANIZATION')\n",
      "372 ('URGENT', 'NNP', 'B-ORGANIZATION')\n",
      "373 ('AJACDEB', 'NNP', 'B-ORGANIZATION')\n",
      "374 ('NEED', 'NNP', 'B-ORGANIZATION')\n",
      "379 ('MEDICATION', 'NNP', 'B-ORGANIZATION')\n",
      "379 ('FOOD', 'NNP', 'B-ORGANIZATION')\n",
      "386 ('PLS', 'NNP', 'B-ORGANIZATION')\n",
      "388 ('HELP', 'NNP', 'B-ORGANIZATION')\n",
      "388 ('PLEA', 'NNP', 'B-ORGANIZATION')\n",
      "411 ('FRENCH', 'NNP', 'B-ORGANIZATION')\n",
      "411 ('SVP', 'NNP', 'B-ORGANIZATION')\n",
      "415 ('Artibonite', 'NNP', 'B-ORGANIZATION')\n",
      "417 ('LOT', 'NNP', 'B-ORGANIZATION')\n",
      "417 ('VOLUNTEERS', 'NNP', 'B-ORGANIZATION')\n",
      "424 ('HELP', 'NNP', 'B-ORGANIZATION')\n",
      "424 ('LOST', 'NNP', 'B-ORGANIZATION')\n",
      "424 ('USE', 'NNP', 'B-ORGANIZATION')\n",
      "426 ('College', 'NNP', 'B-ORGANIZATION')\n",
      "426 ('Louis', 'NNP', 'I-ORGANIZATION')\n",
      "426 ('Mercier', 'NNP', 'I-ORGANIZATION')\n",
      "429 ('USA', 'NNP', 'B-ORGANIZATION')\n",
      "431 ('Digi', 'NNP', 'B-ORGANIZATION')\n",
      "436 ('Justice', 'NNP', 'B-ORGANIZATION')\n",
      "436 ('Palace', 'NNP', 'I-ORGANIZATION')\n",
      "440 ('TENTE', 'NNP', 'B-ORGANIZATION')\n",
      "448 ('Justice', 'NNP', 'B-ORGANIZATION')\n",
      "451 ('Night', 'NNP', 'B-ORGANIZATION')\n",
      "458 ('Brothers', 'NNP', 'B-ORGANIZATION')\n",
      "466 ('Friends', 'NNP', 'B-ORGANIZATION')\n",
      "476 ('CANAPE', 'NNP', 'B-ORGANIZATION')\n",
      "479 ('Cabaret', 'NNP', 'B-ORGANIZATION')\n",
      "484 ('WC', 'NNP', 'B-ORGANIZATION')\n",
      "491 ('Solidarite', 'NNP', 'B-ORGANIZATION')\n",
      "493 ('Aviation', 'NNP', 'B-ORGANIZATION')\n",
      "514 ('Incomplete', 'NNP', 'B-ORGANIZATION')\n",
      "529 ('SOS', 'NNP', 'B-ORGANIZATION')\n",
      "533 ('GOMMIERS', 'NNP', 'B-ORGANIZATION')\n",
      "534 ('Digicel', 'NNP', 'B-ORGANIZATION')\n",
      "543 ('LA', 'NNP', 'B-ORGANIZATION')\n",
      "547 ('MINUSTAH', 'NNP', 'B-ORGANIZATION')\n",
      "556 ('SMS', 'NNP', 'B-ORGANIZATION')\n",
      "570 ('St.', 'NNP', 'B-ORGANIZATION')\n",
      "570 ('Nicolas', 'NNP', 'I-ORGANIZATION')\n",
      "579 ('GOT', 'NNP', 'B-ORGANIZATION')\n",
      "587 ('Hospital', 'NNP', 'B-ORGANIZATION')\n",
      "587 ('Centre', 'NNP', 'I-ORGANIZATION')\n",
      "589 ('Sen', 'NNP', 'B-ORGANIZATION')\n",
      "589 ('Sirin', 'NNP', 'I-ORGANIZATION')\n",
      "589 ('Juvenat', 'NNP', 'I-ORGANIZATION')\n",
      "604 ('Macajoux', 'NNP', 'B-ORGANIZATION')\n",
      "604 ('PLEASE', 'NNP', 'B-ORGANIZATION')\n",
      "604 ('COORDS', 'NNP', 'B-ORGANIZATION')\n",
      "608 ('SOS', 'NNP', 'B-ORGANIZATION')\n",
      "608 ('COUR', 'NNP', 'B-ORGANIZATION')\n",
      "608 ('WATSON', 'NNP', 'B-ORGANIZATION')\n",
      "608 ('PLEASE', 'NNP', 'B-ORGANIZATION')\n",
      "617 ('Come', 'NNP', 'B-ORGANIZATION')\n",
      "617 ('Monseigneur', 'NNP', 'I-ORGANIZATION')\n",
      "617 ('Guilloux', 'NNP', 'I-ORGANIZATION')\n",
      "618 ('SOS', 'NNP', 'B-ORGANIZATION')\n",
      "620 ('Delmas', 'NNP', 'B-ORGANIZATION')\n",
      "635 ('Santo', 'NNP', 'B-ORGANIZATION')\n",
      "637 ('Digicel', 'NNP', 'B-ORGANIZATION')\n",
      "644 ('Gonzague', 'NNP', 'B-ORGANIZATION')\n",
      "648 ('Citron', 'NNP', 'B-ORGANIZATION')\n",
      "662 ('US', 'NNP', 'B-ORGANIZATION')\n",
      "662 ('Visa', 'NNP', 'I-ORGANIZATION')\n",
      "682 ('LOCATED', 'NNP', 'B-ORGANIZATION')\n",
      "682 ('Miragoane', 'NNP', 'I-ORGANIZATION')\n",
      "682 ('PORT', 'NNP', 'B-ORGANIZATION')\n",
      "690 ('Dampus', 'NNP', 'B-ORGANIZATION')\n",
      "691 ('Delmas', 'NNP', 'B-ORGANIZATION')\n",
      "700 ('KIDS', 'NNP', 'B-ORGANIZATION')\n",
      "730 ('Fort', 'NNP', 'B-ORGANIZATION')\n",
      "730 ('Dimanche', 'NNP', 'I-ORGANIZATION')\n",
      "734 ('Tiburon', 'NNP', 'B-ORGANIZATION')\n",
      "794 ('PAP', 'NNP', 'B-ORGANIZATION')\n",
      "806 ('Santo', 'NNP', 'B-ORGANIZATION')\n",
      "809 ('House', 'NNP', 'B-ORGANIZATION')\n",
      "810 ('USA', 'NNP', 'B-ORGANIZATION')\n",
      "811 ('PAP', 'NNP', 'B-ORGANIZATION')\n",
      "824 ('Belanton', 'NNP', 'B-ORGANIZATION')\n",
      "831 ('Fort', 'NNP', 'B-ORGANIZATION')\n",
      "831 ('Jacques', 'NNP', 'I-ORGANIZATION')\n",
      "831 ('HELP', 'NNP', 'B-ORGANIZATION')\n",
      "835 ('PAP', 'NNP', 'B-ORGANIZATION')\n",
      "837 ('HELP', 'NNP', 'B-ORGANIZATION')\n",
      "842 ('UEBH', 'NNP', 'B-ORGANIZATION')\n",
      "842 ('Mercredi', 'NNP', 'B-ORGANIZATION')\n",
      "844 ('Cabaret', 'NNP', 'B-ORGANIZATION')\n",
      "847 ('Minustah', 'NNP', 'B-ORGANIZATION')\n",
      "847 ('Base', 'NNP', 'I-ORGANIZATION')\n",
      "870 ('RUBEN', 'NNP', 'B-ORGANIZATION')\n",
      "923 ('PREGNANT', 'NNP', 'B-ORGANIZATION')\n",
      "928 ('PAP', 'NNP', 'B-ORGANIZATION')\n",
      "929 ('MEDICAL', 'NNP', 'B-ORGANIZATION')\n",
      "944 ('BPH', 'NNP', 'B-ORGANIZATION')\n",
      "944 ('Banque', 'NNP', 'I-ORGANIZATION')\n",
      "944 ('Populaire', 'NNP', 'I-ORGANIZATION')\n",
      "944 ('Haitienne', 'NNP', 'I-ORGANIZATION')\n",
      "947 ('Cite', 'NNP', 'B-ORGANIZATION')\n",
      "947 ('Soleil', 'NNP', 'I-ORGANIZATION')\n",
      "949 ('Hospital', 'NNP', 'B-ORGANIZATION')\n",
      "960 ('Santo', 'NNP', 'B-ORGANIZATION')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968 ('Im', 'NNP', 'B-ORGANIZATION')\n",
      "971 ('DUPONT', 'NNP', 'B-ORGANIZATION')\n",
      "971 ('GOODS', 'NNP', 'I-ORGANIZATION')\n",
      "973 ('DELMAS', 'NNP', 'B-ORGANIZATION')\n",
      "973 ('RUE', 'NNP', 'B-ORGANIZATION')\n",
      "973 ('FOOD', 'NNP', 'B-ORGANIZATION')\n",
      "973 ('TENTS', 'NNP', 'B-ORGANIZATION')\n",
      "982 ('TRANQUILLE', 'NNP', 'B-ORGANIZATION')\n",
      "988 ('Lilavoi', 'NNP', 'B-ORGANIZATION')\n",
      "988 ('La', 'NNP', 'B-ORGANIZATION')\n",
      "988 ('Plaine', 'NNP', 'I-ORGANIZATION')\n",
      "998 ('Association', 'NNP', 'B-ORGANIZATION')\n",
      "998 ('Women', 'NNP', 'I-ORGANIZATION')\n"
     ]
    }
   ],
   "source": [
    "# Let's see what we get in the first 1000 texts\n",
    "for a in range(1000):\n",
    "    checking_orgs(df.message.iloc[a], a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a lot of non organizational names on it. But we still will use this function.\n",
    "# There is space to improve this more making an extra cleaning of these names.\n",
    "# The resulting transforming class to do it:\n",
    "class OrganizationPresence(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    This transforming class will detect, helped by the 'ne_chunk' function, the\n",
    "    presence of an organization's name in the text. That will help us to add\n",
    "    features to our training data.\n",
    "\n",
    "        Internal function:\n",
    "\n",
    "            checking_org :\n",
    "\n",
    "                parameters : text : str ; the text to be searched of an\n",
    "                            organization's names.\n",
    "\n",
    "                returns : True or False ; the presence of an organization's name\n",
    "\n",
    "            fit :\n",
    "\n",
    "                returns : self data, no changes.\n",
    "\n",
    "            transform :\n",
    "\n",
    "                returns : pd.Dataframe ; of a serie of True/False values of an\n",
    "                        organization's names presenced in each text.\n",
    "    '''\n",
    "\n",
    "    # The function that performs really the transformation in this class.\n",
    "    # It will tokenize the words of the text received, delete the stopwords,\n",
    "    # and finally will check, helped by ne_chunk function if the any word\n",
    "    # represent an organization.\n",
    "    def checking_org(self, text):\n",
    "        # First list of words, and cleaning from stopwords.\n",
    "        words = word_tokenize(text)\n",
    "        words = [w for w in words if w.lower() not in stopwords.words('english')]\n",
    "        # Tagging the list.\n",
    "        ptree = pos_tag(words)\n",
    "        # FInally we simplify the tree and check if any word represents an\n",
    "        # organization. This check can be definitvely inproved.\n",
    "        for w in tree2conlltags(ne_chunk(ptree)):\n",
    "            if (w[2][2:] == 'ORGANIZATION') and (w[1] == 'NNP'):\n",
    "                return True\n",
    "\n",
    "            return False\n",
    "    # Fit function with just a structure purpose.\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # The transform function that call to check every text in the input\n",
    "        # series. \n",
    "        X_org = pd.Series(X).apply(self.checking_org)\n",
    "\n",
    "        return pd.DataFrame(X_org)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The other feature we want to add is the length of the texts:\n",
    "class TextLengthExtractor(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    This transforming class will calculate the length of each text in the course\n",
    "    and delivery a dataframe of them.\n",
    "\n",
    "        Internal function:\n",
    "\n",
    "            fit :\n",
    "\n",
    "                returns : self data, no changes.\n",
    "\n",
    "            transform :\n",
    "\n",
    "                returns : pd.Dataframe ; of a serie of numbers representing the\n",
    "                            length of each text.\n",
    "    '''\n",
    "    # Fit function with just a structure purpose.\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # The function that transform the class object in a dataframe with the\n",
    "        # length of every text in the data serie received. \n",
    "        return pd.DataFrame(pd.Series(X).apply(lambda x: len(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then our final pipeline with the best parameters founded.\n",
    "# Implementing Feature Union to add two new features to our data, and\n",
    "# a Random Forest Classifier.\n",
    "pipeline = Pipeline([\n",
    "                        ('features', FeatureUnion([\n",
    "                            # The text pipeline transformers.\n",
    "                            ('text_pipeline', Pipeline([\n",
    "                                ('vect', CountVectorizer(tokenizer=tokenize,\n",
    "                                                         max_df=1.0,\n",
    "                                                         max_features=None,)),\n",
    "                                ('tfidf', TfidfTransformer(norm='l2',\n",
    "                                                           use_idf=True,))\n",
    "                            ])),\n",
    "                            # The new two features added.\n",
    "                            ('org_presence', OrganizationPresence()),\n",
    "                            ('text_length', TextLengthExtractor())\n",
    "                        ])),\n",
    "                        # Our final ML algorithm.\n",
    "                        ('clf', RandomForestClassifier(criterion='gini',\n",
    "                                                        n_estimators=250,\n",
    "                                                        random_state=42,))\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 823.5273387699999\n",
      "0.450668896321\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "               related       0.92      0.99      0.95      4292\n",
      "               request       0.90      0.77      0.83      1099\n",
      "                 offer       0.97      0.71      0.82        51\n",
      "           aid_related       0.86      0.90      0.88      3023\n",
      "          medical_help       0.98      0.49      0.65       602\n",
      "      medical_products       1.00      0.55      0.71       423\n",
      "     search_and_rescue       0.99      0.87      0.93       361\n",
      "              security       1.00      0.90      0.95       229\n",
      "              military       0.95      0.81      0.87       345\n",
      "                 water       0.95      0.64      0.77       473\n",
      "                  food       0.94      0.76      0.84       748\n",
      "               shelter       0.97      0.69      0.80       669\n",
      "              clothing       0.99      0.86      0.92       169\n",
      "                 money       0.99      0.82      0.89       238\n",
      "        missing_people       1.00      0.92      0.96       168\n",
      "              refugees       0.99      0.84      0.91       386\n",
      "                 death       0.97      0.57      0.72       349\n",
      "             other_aid       0.93      0.46      0.62       920\n",
      "infrastructure_related       0.99      0.62      0.76       641\n",
      "             transport       1.00      0.62      0.76       394\n",
      "             buildings       0.98      0.54      0.70       382\n",
      "           electricity       0.99      0.83      0.91       218\n",
      "                 tools       1.00      0.82      0.90        84\n",
      "             hospitals       0.98      0.80      0.88       143\n",
      "                 shops       0.98      0.75      0.85        65\n",
      "           aid_centers       1.00      0.83      0.91       162\n",
      "  other_infrastructure       0.98      0.52      0.68       364\n",
      "       weather_related       0.94      0.85      0.89      1970\n",
      "                floods       0.99      0.70      0.82       647\n",
      "                 storm       0.90      0.69      0.78       609\n",
      "                  fire       0.99      0.83      0.90       141\n",
      "            earthquake       0.96      0.80      0.87       615\n",
      "                  cold       0.99      0.80      0.88       231\n",
      "         other_weather       0.96      0.45      0.61       366\n",
      "         direct_report       0.88      0.67      0.76      1240\n",
      "           not_related       0.72      0.20      0.31       492\n",
      "\n",
      "           avg / total       0.93      0.77      0.83     23309\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same process as before to traing and test the result\n",
    "start = time.perf_counter()\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print('time:', time.perf_counter()-start)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=target_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving finally our model to a pikle file\n",
    "filename = 'Test_model.pkl'\n",
    "pickle.dump(pipeline, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
